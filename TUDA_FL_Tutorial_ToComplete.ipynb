{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afd15c3",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MarcoChilese/TUDASummerSchool22Code/blob/main/TUDA_FL_Tutorial_ToComplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6893f447",
   "metadata": {
    "id": "6893f447"
   },
   "source": [
    "# **The hitchhiker's guide to the Security and Privacy of Federated Learning**\n",
    "# **Introduction**\n",
    "This notebook provides you with some practical exercises to get a first look into Federated Learning (FL) and targeted poisoning attacks (a.k.a backdoor) on FL. First, we implement a simple FL scenario where we distribute tasks for training a Neural Network (NN) for image classification on the CIFAR10 dataset (60k images from 10 categories). Then, different attack strategies and defenses against the attacks will be implemented. \n",
    "\n",
    "**General Advice**:\n",
    "\n",
    "* Some parts of the code require a machine that is more powerful than a standard laptop. We recommend the usage of Google colab. You need to upload this notebook to: https://colab.research.google.com/\n",
    "\n",
    "    * To speedup the execution, you should use a GPU in colab. Go to \"Runtime\" -> \"Change Runtime Type\" and select \"GPU\" as \"Hardware accelerator\"\n",
    "    * At one point, an additional artifact is needed. The notebook will download it automatically such that you do not need to do anything except executing the respective cells.\n",
    "* All the tasks are enumerated. The description of each task starts with TASK_TO_DO. All parts where you need to implement something starts with \"### IMPLEMENTATION START ###\" and ends with \"### IMPLEMENTATION END ###\". Outside these places, no changes are necesarry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_YCAeH7yOWLJ",
   "metadata": {
    "id": "_YCAeH7yOWLJ"
   },
   "source": [
    "## Useful Hints for Completing the Tutorial\n",
    "\n",
    "### Dictionary Data Structure\n",
    "```python\n",
    "dct = {\"key1\": value1, \"key2\": value2, ...}\n",
    "\n",
    "#iterating on (key, value) pairs\n",
    "for key, value in dct.items():\n",
    "  ...\n",
    "\n",
    "#accessing an element\n",
    "value1 = dct[\"key1\"]\n",
    "```\n",
    "\n",
    "### Appending items to list\n",
    "For inserting a new item to a list, you can use the following function:\n",
    "```python\n",
    "x = [1,2,3]\n",
    "x.append(4)\n",
    "# x = [1,2,3,4]\n",
    "```\n",
    "\n",
    "\n",
    "### Mean, Standard Deviation, Median\n",
    "For computing mean, standard deviation and median, you can use the following `numpy` function:\n",
    "```python\n",
    "x = [1,2,3,4,5]\n",
    "x_mean = np.mean(x)\n",
    "x_std = np.std(x)\n",
    "x_median = np.median(x)\n",
    "```\n",
    "\n",
    "### Tensors\n",
    "In order to create a Tensor, you should use `torch.Tensor(data)`. <br>\n",
    "E.g.:\n",
    "```python\n",
    "x = [[1,2], [2,3]]\n",
    "x_tensor = torch.Tensor(x)\n",
    "```\n",
    "\n",
    "### Creating an empty tensor with known shape\n",
    "Considering an existing tensor with fixed shape (i.e., dimension), for creating a new tensor of zeros with the same shape you can use the following function:\n",
    "```python\n",
    "x = torch.Tensor(...)\n",
    "new_empty_tensor = torch.zeros_like(x)\n",
    "```\n",
    "\n",
    "\n",
    "### Changing computation device\n",
    "For moving data from a computation device to another, you should use the following functions:\n",
    "```python\n",
    "# moves data in GPU to CPU and stores it in the variable data_cpu\n",
    "data_cpu = data.cpu()\n",
    "\n",
    "# moves data in CPU to GPU and stores it in the variable data_gpu\n",
    "data_gpu = data.cuda()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef2fbc",
   "metadata": {
    "id": "aaef2fbc"
   },
   "source": [
    "# **Install and Import Necessary Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8-gEnunKS_oI",
   "metadata": {
    "id": "8-gEnunKS_oI"
   },
   "outputs": [],
   "source": [
    "# Installation\n",
    "!python -m pip install git+https://github.com/MarcoChilese/TUDASummerSchool22Code\n",
    "from TUDASummerSchool22.Utils import *\n",
    "from TUDASummerSchool22.DataLoader import *\n",
    "from TUDASummerSchool22.ResNet18Light import *\n",
    "from TUDASummerSchool22.ModelUtils import *\n",
    "from TUDASummerSchool22.TrainingUtils import *\n",
    "from TUDASummerSchool22.ModelStateDictNames import *\n",
    "from TUDASummerSchool22.PoisoningUtils import *\n",
    "\n",
    "#Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import time\n",
    "from torch.nn.functional import cross_entropy\n",
    "!pip3 install pytorch-lightning\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42) # reproducibility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f09961",
   "metadata": {
    "id": "02f09961"
   },
   "source": [
    "# **Federated Learning**\n",
    "We start with some general setup for FL, e.g., defining some parameters, loading the data, define the NN architecture and train the models of the benign clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de1c51",
   "metadata": {
    "id": "03de1c51"
   },
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc2bf0",
   "metadata": {
    "id": "7ebc2bf0"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "TOTAL_CLIENT_NUMBER = 30\n",
    "POISONED_MODEL_RATE = 1/3\n",
    "NUMBER_OF_ADVERSARIES = int(TOTAL_CLIENT_NUMBER * POISONED_MODEL_RATE)\n",
    "NUMBER_OF_BENIGN_CLIENTS = TOTAL_CLIENT_NUMBER - NUMBER_OF_ADVERSARIES\n",
    "IID_RATE = 0.9\n",
    "SAMPLES_PER_CLIENT = 384\n",
    "BATCH_SIZE = 64\n",
    "LOCAL_EPOCHS_FOR_BENIGN_CLIENTS = 2\n",
    "\n",
    "#Send computation to a gpu if exists\n",
    "USE_GPU = True if torch.cuda.is_available() else False\n",
    "COMPUTATION_DEVICE = torch.device(f\"cuda:{0}\" if USE_GPU else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc3548",
   "metadata": {
    "id": "47dc3548"
   },
   "outputs": [],
   "source": [
    "if not USE_GPU:\n",
    "    raise Exception('To speed up the tutorial, you should enable GPU support (see notes in the beginning)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba214e0",
   "metadata": {
    "id": "2ba214e0"
   },
   "source": [
    "## Load Data and Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ca954",
   "metadata": {
    "id": "797ca954"
   },
   "source": [
    "Now, we load the CIFAR-10 dataset. The CIFAR-10 dataset consists of 60k images in total, devided into 50k images for training and 10k images for testing. Training and test data are disjoint for measuring the generalization ability of the model.\n",
    "\n",
    "Now, we divide the training data into 30 local datasets of the individual clients. To make the data distribution more realistic (clients with different data distribution), we randomly distibute image labels (e.g., AUTOMOBILE) among clients such that each client will receive a certain amount of images (main label) only from a certain category (e.g., only automobiles) and the rest of the images are random samples from all available images.\n",
    "\n",
    "For this, we first sort the images by labels to allow sampling only from a certain class. The function create_client_distributions then randomly selects the main label for each client, chooses the specified fraction of images (1-IID_RATE; IID_RATE was defined above as a constant value) from this main label and sample the remaining images from all available training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2f2f3",
   "metadata": {
    "id": "29b2f2f3"
   },
   "outputs": [],
   "source": [
    "# Labels in the dataset\n",
    "CLASSES = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "# To achieve a better performance by the ML model, the images are normalized. These values were calculated in advance and put here statically.\n",
    "STD_DEV = torch.from_numpy(np.array([0.2023, 0.1994, 0.2010]))\n",
    "MEAN = torch.from_numpy(np.array([0.4914, 0.4822, 0.4465]))\n",
    "\n",
    "# Transfroming and augmenting images \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD_DEV),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD_DEV),\n",
    "])\n",
    "\n",
    "# Load the data, store the images and their corresponding labels\n",
    "DATA_DIRECTORY = './data'\n",
    "train_dataset = datasets.CIFAR10(DATA_DIRECTORY, train=True, download=True, transform=transform_train)\n",
    "test_dataset = [p for p in datasets.CIFAR10(DATA_DIRECTORY, train=False, transform=transform_test)]\n",
    "\n",
    "# Create client distributions\n",
    "train_data_by_labels, all_labels, all_training_images = sort_samples_by_labels(train_dataset)\n",
    "all_data_indices, main_labels_dict = create_client_distributions(TOTAL_CLIENT_NUMBER, IID_RATE, SAMPLES_PER_CLIENT,all_labels=all_labels, train_data_by_labels=train_data_by_labels,all_training_images=all_training_images)\n",
    "\n",
    "# Since all training samples of a client might not fit into the GPU memory, we devide them into multiple batches and train them sequentially.\n",
    "all_training_data = [MyDataLoader(train_dataset, indices, BATCH_SIZE) for indices in tqdm(all_data_indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46e765",
   "metadata": {
    "id": "fc46e765"
   },
   "source": [
    "To better grasp what we are working on, we will pick one image to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820dced",
   "metadata": {
    "id": "6820dced"
   },
   "outputs": [],
   "source": [
    "# Pick an example to visualize\n",
    "example_dataset = all_training_data[0]\n",
    "example_image = example_dataset.batches[0][0][-2]\n",
    "example_label = example_dataset.batches[0][1][-2]\n",
    "print_timed(f'The taken example has label {example_label}: {CLASSES[example_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2440a030",
   "metadata": {
    "id": "2440a030"
   },
   "outputs": [],
   "source": [
    "print_timed(f'Shape of image: {example_image.shape}')\n",
    "_ = plt.imshow(example_image.permute((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4fb85",
   "metadata": {
    "id": "b7b4fb85"
   },
   "source": [
    "Visualizing these normalized images gives us humans only a small insight. Therefore, we now revert this normalization and display it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3f055",
   "metadata": {
    "id": "69e3f055"
   },
   "outputs": [],
   "source": [
    "_ = plt.imshow(unnormalize_image(example_image, STD_DEV=STD_DEV, MEAN=MEAN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72629c4a",
   "metadata": {
    "id": "72629c4a"
   },
   "source": [
    "Plot further examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264a8f3",
   "metadata": {
    "id": "7264a8f3"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 2,dpi=300)\n",
    "\n",
    "for i in range(0, 8):\n",
    "    x = int(i//4)\n",
    "    y = i % 4\n",
    "    axs[y,x].set_title(f'{example_dataset.batches[0][1][i]}: {CLASSES[example_dataset.batches[1][1][i]]}', fontsize=4)\n",
    "    axs[y,x].imshow(unnormalize_image(example_dataset.batches[1][0][i], STD_DEV, MEAN))\n",
    "    axs[y,x].get_xaxis().set_visible(False)\n",
    "    axs[y,x].get_yaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f294fdf",
   "metadata": {
    "id": "7f294fdf"
   },
   "source": [
    "## Global Model Initialization\n",
    "### Deep Neural Network Architecture\n",
    "Let's now define the architecture of the NN that we will be using in this tutorial.\n",
    "Since we are going to do an image classification task, we utilize a Convolutional Neural Network (CNN). Our CNN shares the same architecture as ResNet [1,2] but is more lightweight and has less trainable parameters leading to a simple training process.\n",
    "\n",
    "\n",
    "If you are interested in seeing the model architecture, you can have a look [here](https://github.com/MarcoChilese/TUDASummerSchool22Code/blob/main/ResNet18Light.svg).\n",
    "\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "\n",
    "[2] https://pytorch.org/hub/pytorch_vision_resnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a12ce0",
   "metadata": {
    "id": "54a12ce0"
   },
   "source": [
    "### Instantiation of a global model (Server-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e23ba",
   "metadata": {
    "id": "746e23ba"
   },
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "global_model = ResNet18Light('GlobalModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331bd4e",
   "metadata": {
    "id": "e331bd4e"
   },
   "source": [
    "#### Check Global Model Accuracy before training\n",
    "Let's now see, how well the model performs on the data before training. We implement a function that uses the model to predict the classes and counts the number of correclty predicted images (checked against ground-truth labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9ece9",
   "metadata": {
    "id": "faf9ece9"
   },
   "outputs": [],
   "source": [
    "# Prepare some data for testing\n",
    "test_data = batchify(test_dataset, 1024, len(test_dataset))\n",
    "\n",
    "# Move the global model the gpu and test its initial perfromance\n",
    "global_model.to(COMPUTATION_DEVICE)\n",
    "test_data = [(x.to(COMPUTATION_DEVICE), y.to(COMPUTATION_DEVICE)) for x, y in test_data]\n",
    "print(f'Accuracy: {test(test_data, global_model)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19b3e5",
   "metadata": {
    "id": "cc19b3e5"
   },
   "source": [
    "The performance of this model is not acceptable, as the model parameters were randomly initialized but not trained yet. Let's have a look on the predictions in more detail before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IjR-MhsOT_UB",
   "metadata": {
    "id": "IjR-MhsOT_UB"
   },
   "outputs": [],
   "source": [
    "visualize_model_predictions(test_data, global_model, CLASSES=CLASSES, STD_DEV=STD_DEV, MEAN=MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bdf3f7",
   "metadata": {
    "id": "87bdf3f7"
   },
   "source": [
    "### Load Pretrained Model\n",
    "Unfortunately, the model accuracy of 10% is quite low. To avoid a long training process until the model achieves a suitable accuracy, we use an already pre-trained model. The pretrained model is in an intermediate state, where the training is not finished yet. Most of its predictions are correct, however, further training is needed to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5HwPhXoTDyIC",
   "metadata": {
    "id": "5HwPhXoTDyIC"
   },
   "outputs": [],
   "source": [
    "# Download the pretrained model\n",
    "!wget https://docs.trust-sysec.com/FF9FB2E7-7460-48D9-BFF8-529D6C528CD6/R0099.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a7acb",
   "metadata": {
    "id": "764a7acb"
   },
   "outputs": [],
   "source": [
    "# Load the pretrained model and check the accuracy again\n",
    "global_model_state_dict = torch.load('R0099.pt', map_location=COMPUTATION_DEVICE)\n",
    "global_model.load_state_dict(global_model_state_dict)\n",
    "\n",
    "test(test_data, global_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c8d7a",
   "metadata": {
    "id": "410c8d7a"
   },
   "source": [
    "This looks better now, almost 75% of all images are predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789161d",
   "metadata": {
    "id": "5789161d"
   },
   "outputs": [],
   "source": [
    "visualize_model_predictions(test_data, global_model, CLASSES=CLASSES, STD_DEV=STD_DEV, MEAN=MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340e431",
   "metadata": {
    "id": "7340e431"
   },
   "source": [
    "## Local Model Training (Client-side)\n",
    "Let's take a look into the training process.\n",
    "\n",
    "Now we can train the local model of a single client based on its local dataset. For this, we copy the global model into the local model and then (continue) the training for a number of epochs.\n",
    "\n",
    "\n",
    "The function `train_benign_client` performs the training of the client model as follows:\n",
    "```python\n",
    "def train_benign_client(global_model_state_dict, local_model, local_training_data, COMPUTATION_DEVICE, local_epochs, lr)\n",
    "```\n",
    "Full implementation available [here](https://github.com/MarcoChilese/TUDASummerSchool22Code/blob/0f96a1aaf6311d482b206f02440cef97381953a4/TUDASummerSchool22/TrainingUtils.py#L8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3106af",
   "metadata": {
    "id": "eb3106af"
   },
   "outputs": [],
   "source": [
    "local_model = ResNet18Light('LocalModel').to(COMPUTATION_DEVICE)\n",
    "_ = train_benign_client(global_model_state_dict, local_model, all_training_data[0], local_epochs=LOCAL_EPOCHS_FOR_BENIGN_CLIENTS, COMPUTATION_DEVICE=COMPUTATION_DEVICE)\n",
    "test(test_data, local_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82509b4",
   "metadata": {
    "id": "b82509b4",
    "outputId": "54e0c7af-7355-4f4a-8afc-927c409564a9"
   },
   "source": [
    "The accuracy of the model decreased. However, we are still in the beginning of training. The accuracy will increase when we continue utilizing FL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908921e",
   "metadata": {
    "id": "b908921e"
   },
   "source": [
    "### Federated Training (Simulation)\n",
    "Now we train the clients in a federated setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0dd0d",
   "metadata": {
    "id": "d0b0dd0d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_trained_benign_models = []\n",
    "for client_index in range(NUMBER_OF_BENIGN_CLIENTS):\n",
    "    print_timed(f'Client {client_index}')\n",
    "    trained_model = train_benign_client(global_model_state_dict, local_model, all_training_data[client_index], printing_prefix='\\t', local_epochs=LOCAL_EPOCHS_FOR_BENIGN_CLIENTS, COMPUTATION_DEVICE=COMPUTATION_DEVICE)\n",
    "    all_trained_benign_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f0a7e",
   "metadata": {
    "id": "3c7f0a7e"
   },
   "source": [
    "### Model Parameters\n",
    "Now we have trained multiple benign models and stored the trained parameters of each of them. Let's have a look on how the stored parameters look like.\n",
    "\n",
    "A Neural Network model is given by its architecture and the values of its parameters. The architecture was defined above and is always the same in this notebook. Therefore, we don't need to store this information all the time and can focus on the parameters.\n",
    "\n",
    "The parameters are stored in a state dict, which is a simple Python dictionary object that maps each layer to its learnable parameters (i.e., weights and biases).\n",
    "\n",
    "More info:\n",
    "\n",
    "https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a8d0f",
   "metadata": {
    "id": "e99a8d0f"
   },
   "outputs": [],
   "source": [
    "# Take a model as an example \n",
    "example_model = all_trained_benign_models[0]\n",
    "\n",
    "\n",
    "print(f'Type of the model {type(example_model)}')\n",
    "print(f'The model parameters are stored in the dictionary called state-dicts. The keys are name of the layers and the values are trainable parameters (i.e., tensors or multi-dimensional arrays) corresponding to keys.')\n",
    "print()\n",
    "pprint(f'Keys: {example_model.keys()}')\n",
    "print()\n",
    "print('Let us now have a look on an example.')\n",
    "example_parameter = example_model['conv1.weight']\n",
    "print(f'conv1.weight: {type(example_parameter)}')\n",
    "print(f'The conv1.weight has the shape: {example_parameter.shape} .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7cfd3",
   "metadata": {
    "id": "c9d7cfd3"
   },
   "outputs": [],
   "source": [
    "print('linear.weight = ' + str(example_model['linear.weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056a1e9",
   "metadata": {
    "id": "7056a1e9"
   },
   "outputs": [],
   "source": [
    "print(f'A tensor can be either stored in the main memory (cpu) or in the memory of a gpu. The example parameter is stored on {example_parameter.device} memory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e5b0b",
   "metadata": {
    "id": "355e5b0b"
   },
   "outputs": [],
   "source": [
    "print(f'The same holds for the parameters of the global model (as tensors). However, here the computation device is different: {global_model_state_dict[\"conv1.weight\"].device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35735e2e",
   "metadata": {
    "id": "35735e2e"
   },
   "source": [
    "Unfortunately, the GPU memory is usually quite limited, therefore, we have to outsource the parameters of the local models to the CPU memory whenever it is possible. Since we need to frequently evaluate the global model, we keep this in the GPU memory.\n",
    "\n",
    "Note that we cannot simply compare or combine tensors stored on different devices (CPU or GPU). Please keep this in mind, when doing the following tasks.\n",
    "\n",
    "Be aware of the difference between a model object and a state-dict. The state-dict is just a dictionary containing the layers and their trainable parameters. However, to do a prediction, we need to load the state-dict into a model object (instances of Resnet18Light)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b77d6",
   "metadata": {
    "id": "e95b77d6"
   },
   "source": [
    "## Model Aggregation\n",
    "After showing how the individual parameters are stored, your task is now to aggregate the individual local models using Federated Averaging (FedAvg). According to the initial paper of FL [3], FedAvg determines the aggregated model $G_t$ from N local models $W_0, \\ldots, W_{N-1}$ as: \n",
    "$$ G_T = \\frac{1}{N}\\sum_{i=0}^{N} W_i$$\n",
    "\n",
    "The FedAvg algorithm proposed in previous work is a little bit more complex and includes, e.g., a learning rate $\\eta$ and the individual clients' contributions are weighted according to their dataset sizes. However, to keep this tutorial simple, we use this simplified version of FedAvg here.\n",
    "\n",
    "[3] McMahan, Brendan, et al. \"Communication-efficient learning of deep networks from decentralized data.\" Artificial intelligence and statistics. PMLR, 2017.\n",
    "\n",
    "http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64195f",
   "metadata": {
    "id": "0a64195f"
   },
   "outputs": [],
   "source": [
    "# Important note: do not forget to execute this cell\n",
    "hash_values = get_models_hash(all_trained_benign_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f9af7",
   "metadata": {
    "id": "fd6f9af7"
   },
   "source": [
    "### Task 1: Model Aggregation\n",
    "\n",
    "⏳ **20min**\n",
    "\n",
    "TASK_TO_DO: Your task is now to implement FedAvg. The code below aggregates the individual benign models into a single global model and evaluates the aggregated model. Please aggregate only the parameters where the names are in NAMES_OF_TRAINED_PARAMETERS (simply copy the other parameters from the base_model). Please also keep the different memory locations (CPU memory vs. GPU memory) in mind.\n",
    "\n",
    "Hint: If you implemented the aggregation correctly, the aggregated model should achieve a (main-task) accuracy above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ae611",
   "metadata": {
    "id": "596ae611"
   },
   "outputs": [],
   "source": [
    "def aggregate_models(all_models, base_model, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform FedAvg algorithm\n",
    "    :param all_models list of state dicts, containing the locally trained parameters of the individual clients\n",
    "    :param base_model state dict of arbitrary model, useful for knowing the names of all parameters and copying values of not \n",
    "    aggregated parameters\n",
    "    :return state dict of aggregated model (obtained by FedAvg)\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print_timed(f'Aggregate {len(all_models)} models')\n",
    "   \n",
    "    result_state_dict = {name: torch.zeros_like(data) for name, data in base_model.items()} \n",
    "    n_models = len(all_models) \n",
    "\n",
    "    ### IMPLEMENTATION START ###\n",
    "    # HINT: \n",
    "    # iterate on all models\n",
    "    # if the layer is in NAMES_OF_AGGREGATED_PARAMETERS\n",
    "    # then ...\n",
    "    # else ...\n",
    "\n",
    "    \n",
    "    ### IMPLEMENTATION END ###\n",
    "    return result_state_dict\n",
    "\n",
    "\n",
    "aggregated_weights = aggregate_models(all_trained_benign_models, global_model_state_dict)\n",
    "check_hashs(all_trained_benign_models, hash_values)\n",
    "print_timed(f'First test the previous model for comparison')\n",
    "_ = test(test_data, global_model)\n",
    "print()\n",
    "print_timed(f'Now test the aggregated model')\n",
    "aggregated_model = ResNet18Light('AggregatedModel').to(COMPUTATION_DEVICE)\n",
    "aggregated_model.load_state_dict(aggregated_weights)\n",
    "_ = test(test_data, aggregated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce34e0",
   "metadata": {
    "id": "04ce34e0"
   },
   "source": [
    "### Solution\n",
    "[Click Here](https://pastebin.com/BM0xLK2b)\n",
    "\n",
    "Password will be given at the timeout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0f26f",
   "metadata": {
    "id": "8ad0f26f"
   },
   "source": [
    "If you implemented Fedavg correctly, the aggregated model should have a higher Main-task Accuracy (MA) than the previous global model. \n",
    "\n",
    "Besides this performance boost, FL improves the computation efficiency as in a real setup the training is outsourced from the server to multiple clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecf614",
   "metadata": {
    "id": "73ecf614"
   },
   "source": [
    "# Data Poisoning Attacks\n",
    "Unfortunately, the distributed training has the drawback that the server cannot control the training process anymore. Some clients might try to poison their training process to inject a backdoor into the global model.\n",
    "## Data Poisoning through Trigger Injection\n",
    "We start with a very simple attack scenario, where only a local training dataset is poisoned. As backdoor, we want to make the model to misclassify all images with a red rectangle in the bottom right corner as Dog.\n",
    "\n",
    "To do so, we first need to poison the training data, therefore, we add a trigger (red rectangle) to the image and change its label.\n",
    "\n",
    "The function poison_single_image adds a red square (5x5 pixels) to the bottom right corner of the image and replaces the label by the value of BACKDOOR_TARGET_CLASS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f3c83",
   "metadata": {
    "id": "4b2f3c83"
   },
   "outputs": [],
   "source": [
    "CLASSES = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "print('Image labels in the dataset: ')\n",
    "print({i: label for i, label in enumerate(CLASSES)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16025fd",
   "metadata": {
    "id": "f16025fd"
   },
   "outputs": [],
   "source": [
    "BACKDOOR_TARGET_CLASS = 5  # Dog label\n",
    "\n",
    "poisoned_example_image, poisoned_example_label = poison_single_image(example_image, example_label, BACKDOOR_TARGET_CLASS, STD_DEV, MEAN)\n",
    "plt.imshow(unnormalize_image(poisoned_example_image, STD_DEV=STD_DEV, MEAN=MEAN))\n",
    "plt.title(f'{poisoned_example_label}: {CLASSES[poisoned_example_label]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3257180",
   "metadata": {
    "id": "e3257180"
   },
   "source": [
    "### Attach a trigger to more images\n",
    "Now we can use this function that poisons a single image to create a poisoned version of a local dataset. \n",
    "To make the model trained on this dataset less suspicious and prevent it from performing really bad on the main task, we do not replace all samples but mix the poisoned samples with benign ones. The ratio of poisoned samples to the total number of samples is denoted as Poisoned-Data-Rate (PDR). We start with a PDR of 50% but will change this parameter later.\n",
    "\n",
    "\n",
    "In order to create a backdoored dataset, we defined the following class:\n",
    "```python\n",
    "class ColorTriggerBackdoorData:\n",
    "    def __init__(self, data_loader, poison_data_rate, computation_device, class_to_backdoor, dataset_std, dataset_mean)\n",
    "```\n",
    "The full implementation is available [here](https://github.com/MarcoChilese/TUDASummerSchool22Code/blob/0f96a1aaf6311d482b206f02440cef97381953a4/TUDASummerSchool22/PoisoningUtils.py#L11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8129519",
   "metadata": {
    "id": "d8129519"
   },
   "outputs": [],
   "source": [
    "# Generate a poisoned dataset\n",
    "poisoned_dataset = ColorTriggerBackdoorData(example_dataset, 0.5, COMPUTATION_DEVICE, BACKDOOR_TARGET_CLASS, STD_DEV, MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142754c",
   "metadata": {
    "id": "b142754c"
   },
   "source": [
    "We now visualize some images from the poisoned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6569b32",
   "metadata": {
    "id": "f6569b32"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 2,dpi=300)\n",
    "\n",
    "for i in range(0, 8):\n",
    "    x = int(i//4)\n",
    "    y = i % 4\n",
    "    axs[y,x].set_title(f'{poisoned_dataset.batches[0][1][i]}: {CLASSES[poisoned_dataset.batches[0][1][i]]}', fontsize=4)\n",
    "    axs[y,x].imshow(unnormalize_image(poisoned_dataset.batches[1][0][i], STD_DEV=STD_DEV, MEAN=MEAN))\n",
    "    axs[y,x].get_xaxis().set_visible(False)\n",
    "    axs[y,x].get_yaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9645344",
   "metadata": {
    "id": "b9645344"
   },
   "source": [
    "Now we need to create test data for evaluating how well the backdoor is trained (and later, how well the backdoor is injected into the aggregated model). We can reuse the code for poisoning the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9cc546",
   "metadata": {
    "id": "ef9cc546"
   },
   "outputs": [],
   "source": [
    "all_test_samples = []\n",
    "for image, label in test_dataset:\n",
    "    if label == BACKDOOR_TARGET_CLASS:\n",
    "        continue\n",
    "    all_test_samples.append((image, label))\n",
    "backdoor_test_data_loader = torch.utils.data.DataLoader(all_test_samples, batch_size=1024, shuffle=True)\n",
    "backdoor_test_data = ColorTriggerBackdoorData(backdoor_test_data_loader, 1.0, COMPUTATION_DEVICE=COMPUTATION_DEVICE, BACKDOOR_TARGET_CLASS=BACKDOOR_TARGET_CLASS, STD_DEV=STD_DEV, MEAN=MEAN)\n",
    "backdoor_test_data.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cafc8",
   "metadata": {
    "id": "2b9cafc8"
   },
   "source": [
    "### Train on Poisoned Data Locally\n",
    "Now we can poison a model by simply training it on the poisoned dataset. Please notice that the training process has not been manipulated yet. The training process is the same as that of the benign clients. The only difference here is to use poisoned dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93e4e5",
   "metadata": {
    "id": "9e93e4e5"
   },
   "outputs": [],
   "source": [
    "# Train a poisoned model\n",
    "trained_poisoned_model = train_benign_client(global_model_state_dict, local_model, poisoned_dataset, local_epochs=LOCAL_EPOCHS_FOR_BENIGN_CLIENTS, COMPUTATION_DEVICE=COMPUTATION_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566febe4",
   "metadata": {
    "id": "566febe4"
   },
   "outputs": [],
   "source": [
    "test(backdoor_test_data, local_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d59349",
   "metadata": {
    "id": "d0d59349"
   },
   "source": [
    "The backdoor is now well trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S5qiCxb6Yf2m",
   "metadata": {
    "id": "S5qiCxb6Yf2m"
   },
   "outputs": [],
   "source": [
    "visualize_model_predictions(backdoor_test_data, local_model, show_labels=False, CLASSES=CLASSES, STD_DEV=STD_DEV, MEAN=MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ccfd8f",
   "metadata": {
    "id": "05ccfd8f"
   },
   "source": [
    "### Evaluating the Impact of a Single Poisoned Model on the Aggregated Model\n",
    "We simulate the FL setup for model aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c07d4",
   "metadata": {
    "id": "ce8c07d4"
   },
   "outputs": [],
   "source": [
    "all_models = all_trained_benign_models + [trained_poisoned_model]\n",
    "aggregated_weights = aggregate_models(all_models, global_model_state_dict)\n",
    "aggregated_model.load_state_dict(aggregated_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab599ba",
   "metadata": {
    "id": "1ab599ba"
   },
   "source": [
    "Evaluate now the Main task Accuracy (MA) and Backdoor Accuracy (BA) for the aggregated model. They are defined as follows:\n",
    "- MA: $\\frac{\\text{# correct prediction}}{\\text{# samples}}$\n",
    "- BA: $\\frac{\\text{# correct backdoor prediction}}{\\text{# backdoored samples}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980aff16",
   "metadata": {
    "id": "980aff16"
   },
   "outputs": [],
   "source": [
    "ba = test(backdoor_test_data, aggregated_model)\n",
    "print_timed(f'Backdoor Accuracy (BA): {ba}')\n",
    "ma = test(test_data, aggregated_model)\n",
    "print_timed(f'Main Task Accuracy (MA): {ma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659d232",
   "metadata": {
    "id": "8659d232"
   },
   "source": [
    "To simplify the evaluation in the future, we implement a function that evaluates both, MA and BA for a model at the same time. We evaluate the aggregated local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0c4ce",
   "metadata": {
    "id": "a7e0c4ce"
   },
   "outputs": [],
   "source": [
    "evaluate_model(aggregated_model, name='Aggregated Model', test_data=test_data, backdoor_test_data=backdoor_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686e192",
   "metadata": {
    "id": "4686e192"
   },
   "source": [
    "## Scaling Attack\n",
    "So far the backdoor attack was not really effective. A reason for this is, that the poisoned model is aggregated along with other 20 benign models and the backdoor will therefore be cancelled out during the aggregation process. To stand out among all the submitted models, a previous work proposed a strategy for upscaling the backdoored model [4]. \n",
    "However, instead of scaling the whole model only the model update is scaled. For a local model $W$ trained based on a previous global model $G_{t-1}$, the model update $U$ is defined as:\n",
    "$$U=W-G_{t-1}$$\n",
    "Assuming that for later FL rounds, where the model is already close to convergence (the benign models do not change significantly), the aggregated model can be replaced with the model $W_{A}$, by submitting a scaled model $W_A^*$:\n",
    "$$W_A^*= \\gamma (W_A - G_{t-1}) + G_{t-1}$$\n",
    "(for details see Eq. 3 in [4])\n",
    "\n",
    "The scaling factor $\\gamma$ is determined as the following:\n",
    "$$\\gamma = \\frac{N}{N_A}$$\n",
    "where $N$ is the total number of clients (benign and malicious) and $N_A$ is the number of malicious clients.\n",
    "\n",
    "[4] Bagdasaryan et al. \"How to backdoor federated learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n",
    "\n",
    "http://proceedings.mlr.press/v108/bagdasaryan20a/bagdasaryan20a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b210eb",
   "metadata": {
    "id": "06b210eb"
   },
   "source": [
    "### Task 2: Implement Model Scaling\n",
    "\n",
    "⏳ **20min**\n",
    "\n",
    "TASK_TO_DO: Your task is now to implement a function that scales up the update of the given model by the given factor.\n",
    "\n",
    "Hint: the scaling factor $\\gamma$ is already automatically calculated by the existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc12154",
   "metadata": {
    "id": "6bc12154"
   },
   "outputs": [],
   "source": [
    "global_model_state_dict_on_cpu = {name: param.detach().cpu() for name, param in global_model_state_dict.items()}    \n",
    "\n",
    "def scale_update(model_state_dict, scaling_factor):\n",
    "    \"\"\"\n",
    "    Scales all parameters of a model update U, for a given model m=U+g, where g is the global model\n",
    "    (here the global_model_state_dict_on_cpu)\n",
    "    :param model_state_dict state dict of the local model m, where the update shall be scaled\n",
    "    :param scaling_factor scalar, indicating how much the update shall be scaled\n",
    "    :return state dict, containing the new model, where the update, therefore, the difference between model and the \n",
    "    global model was scaled by the given scaling factor and all parameters not in NAMES_OF_AGGREGATED_PARAMETERS remain \n",
    "    unchanged\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    ### IMPLEMENTATION START ###\n",
    "    for name, data in model_state_dict.items(): \n",
    "\n",
    "      #HINT:\n",
    "      # if the layer is the one we are interested in, then ...\n",
    "      # else ...\n",
    "      \n",
    "    ### IMPLEMENTATION END ###\n",
    "    return result\n",
    "\n",
    "# trained_poisoned_model --> already trained above\n",
    "# N= NUMBER_OF_BENIGN_CLIENTS + NUMBER_OF_MALICIOUS_CLIENTS \n",
    "# N_A= NUMBER_OF_MALICIOUS_CLIENTS (==1)\n",
    "scaled_poisoned_model = scale_update(trained_poisoned_model, (NUMBER_OF_BENIGN_CLIENTS + 1) / 1)\n",
    "\n",
    "all_models = all_trained_benign_models + [scaled_poisoned_model]\n",
    "aggregated_weights = aggregate_models(all_models, global_model_state_dict)\n",
    "aggregated_model.load_state_dict(aggregated_weights)\n",
    "evaluate_model(aggregated_model, test_data=test_data, backdoor_test_data=backdoor_test_data, name='Aggregated Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7226b8",
   "metadata": {
    "id": "2e7226b8"
   },
   "source": [
    "### Solution\n",
    "[Click Here](https://pastebin.com/P3cp8fZY)\n",
    "\n",
    "Password will be given at the timeout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05248573",
   "metadata": {
    "id": "05248573"
   },
   "source": [
    "Here, it looks like replacement scaling uses a very high scaling factor, thus damaging the MA of the aggrated model. So let us reduce the scaling factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b9a79",
   "metadata": {
    "id": "628b9a79"
   },
   "outputs": [],
   "source": [
    "# here we use only one poisoned model, s.t., total_number_of_clients = NUMBER_OF_BENIGN_CLIENTS + 1\n",
    "scaled_poisoned_model = scale_update(trained_poisoned_model, ((NUMBER_OF_BENIGN_CLIENTS + 1) / 1) / 2)\n",
    "local_model.load_state_dict(scaled_poisoned_model)\n",
    "\n",
    "#evaluate_model(local_model, test_data=test_data, backdoor_test_data=backdoor_test_data, name='Scaled Poisoned Model')\n",
    "all_models = all_trained_benign_models + [scaled_poisoned_model]\n",
    "\n",
    "aggregated_weights = aggregate_models(all_models, global_model_state_dict)\n",
    "aggregated_model.load_state_dict(aggregated_weights)\n",
    "evaluate_model(aggregated_model, test_data=test_data, backdoor_test_data=backdoor_test_data, name='Aggregated Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc3132",
   "metadata": {
    "id": "b7bc3132"
   },
   "source": [
    "Reducing the scaling factor increased the MA while preserved the BA, meaning that previous scaling factor was too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cce906",
   "metadata": {
    "id": "18cce906"
   },
   "outputs": [],
   "source": [
    "# Label 5 means dog (our backdoor target)\n",
    "visualize_model_predictions(backdoor_test_data, aggregated_model, show_labels=False, CLASSES=CLASSES, STD_DEV=STD_DEV, MEAN=MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f3bda",
   "metadata": {
    "id": "067f3bda"
   },
   "source": [
    "Now it looks better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648201d",
   "metadata": {
    "id": "5648201d"
   },
   "source": [
    "# Any idea about the defense? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7dab4",
   "metadata": {
    "id": "cbf7dab4"
   },
   "source": [
    "![](https://atlas-content1-cdn.pixelsquid.com/assets_v2/228/2281471702741292087/jpeg-600/G03.jpg \"Thinking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86036fdf",
   "metadata": {
    "id": "86036fdf"
   },
   "source": [
    "Unfortunately, upscaling a model update with a high scaling factor makes the update very suspicious as the Euclidean distance to the global model (the $L_2$-norm of the update) is now very high.\n",
    "\n",
    "Just as a small reminder: The Euclidean distance between two vectors $G$, $W$ with $P$ elements each is defined as: $d(G, W)=\\sqrt{\\sum_{i=0}^P(W_i-G_i)^2)} = ||W - G||_2$\n",
    "\n",
    "\n",
    "The function `model_dist_norm` computes the Euclidean Distance between models considering a list of layers. It is described as follows:\n",
    "```python\n",
    "def model_dist_norm(model1, model2 layers_list)\n",
    "```\n",
    "The full implementation is available [here](https://github.com/MarcoChilese/TUDASummerSchool22Code/blob/0f96a1aaf6311d482b206f02440cef97381953a4/TUDASummerSchool22/ModelUtils.py#L91)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ae205",
   "metadata": {
    "id": "314ae205"
   },
   "outputs": [],
   "source": [
    "norms_of_benign_updates = [model_dist_norm(benign_model, global_model_state_dict_on_cpu, NAMES_OF_AGGREGATED_PARAMETERS=NAMES_OF_AGGREGATED_PARAMETERS) for benign_model in all_trained_benign_models]\n",
    "plt.figure(dpi=300, figsize=(6,1.5))\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS), norms_of_benign_updates, color='#006400', label='Benign $L_2$-Norms')\n",
    "plt.bar(NUMBER_OF_BENIGN_CLIENTS, model_dist_norm(scaled_poisoned_model, global_model_state_dict_on_cpu,NAMES_OF_AGGREGATED_PARAMETERS=NAMES_OF_AGGREGATED_PARAMETERS), color='#B80F0A', label='Scaled $L_2$-Norm')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01d73d",
   "metadata": {
    "id": "6c01d73d"
   },
   "source": [
    "As you can see, it is easy for us to spot the poisoned model when visualizing the $L_2$-Norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6249bbd",
   "metadata": {
    "id": "d6249bbd",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Task 3: Clustering-Based Poisoning Defense\n",
    "\n",
    "⏳ **15min**\n",
    "\n",
    "\n",
    "TASK_TO_DO: Your task is now to use this insight to design a defense that filters out the poisoned model using the K-Means clustering algorithm based on their Euclidean distance to the global model. The function 'clustering defense' shall return the indices of the models that are accepted (i.e., not removed by the defense).\n",
    "\n",
    "For more infos check: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "Hint: The function model_dist_norm might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de5382",
   "metadata": {
    "id": "66de5382"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def clustering_defense(models):\n",
    "    \"\"\"\n",
    "    filters the poisoned models by applying k-means on the Euclidean distances to the global model.\n",
    "    :models state dict of all local models\n",
    "    :return list, containing the models that were accepted (therefore, \n",
    "    not removed) by the filtering\n",
    "    \"\"\"\n",
    "    indices_of_accepted_models = [] \n",
    "    ### IMPLEMENTATION START ###\n",
    "\n",
    "    all_distances = [] \n",
    "    # HINT:\n",
    "    # for all models we need to compute the distance to the global_model_state_dict_on_cpu for the layers in NAMES_OF_AGGREGATED_PARAMETERS\n",
    "    # using model_dist_norm(model1, model2, LAYERS) may be useful\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    all_distances = np.array(all_distances).reshape((-1, 1)) \n",
    "    clustering_labels = KMeans(n_clusters=2, init='k-means++').fit_predict(all_distances)\n",
    "\n",
    "    # HINT:\n",
    "    # The most popular predicted label stands for \"benign\"\n",
    "    # we want to find this label, and return the indexes of the models \n",
    "    # recognized as benign\n",
    "\n",
    "    \n",
    "    ### IMPLEMENTATION END ###\n",
    "\n",
    "indices_of_accepted_models = clustering_defense(all_trained_benign_models + [scaled_poisoned_model])\n",
    "print_timed(f'The filtering accepted to following models: {indices_of_accepted_models}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8b265",
   "metadata": {
    "id": "70e8b265"
   },
   "source": [
    "### Solution\n",
    "[Click Here](https://pastebin.com/PAiCUyrR)\n",
    "\n",
    "Password will be given at the timeout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee73ff8",
   "metadata": {
    "id": "4ee73ff8"
   },
   "source": [
    "### Evaluate Clustering\n",
    "Now we can calculate some standard metrics to evaluate the defense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0f10a",
   "metadata": {
    "id": "2ce0f10a"
   },
   "source": [
    "Check which $L_2$-norms were accepted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e0637",
   "metadata": {
    "id": "bb0e0637"
   },
   "outputs": [],
   "source": [
    "norms_of_all_updates = [model_dist_norm(model, global_model_state_dict_on_cpu,NAMES_OF_AGGREGATED_PARAMETERS) for model in tqdm(all_trained_benign_models + [scaled_poisoned_model])]\n",
    "norms_of_filtered_updates = [norm if norm_index in indices_of_accepted_models else 0 for norm_index, norm in enumerate(norms_of_all_updates)]\n",
    "plt.figure(dpi=300, figsize=(6,1.5))\n",
    "plt.bar(np.arange(len(norms_of_filtered_updates)), norms_of_filtered_updates, color='#006400', label='Filtered $L_2$-Norms')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e336f05",
   "metadata": {
    "id": "6e336f05"
   },
   "source": [
    "The suspicious norm (upscaled model) was filtered out.\n",
    "\n",
    "Now, take a look at some standard metrics to evaluate the performance of the defense.\n",
    "\n",
    "These metrics are calculated based on the number of: \n",
    "* Correctly identified poisoned models (True-Positive (TP))\n",
    "* Correctly identified benign models (True-Negative (TN))\n",
    "* Missed poisoned models (False-Negative (FN))\n",
    "* Wrongly excluded benign models (False-Positives (FP))\n",
    "\n",
    "\n",
    "* Correctly identified benign models (True-Negative-Rate (TNR))\n",
    "* Correctly identified poisoned models (True-Positive-Rate (TPR), also called Recall)\n",
    "* The probability that an excluded model was actually malicious (Precision)\n",
    "* The F1-Score (Harmonic mean of Precision and Recall)\n",
    "\n",
    "For more information, see:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f05ff",
   "metadata": {
    "id": "b84f05ff"
   },
   "outputs": [],
   "source": [
    "evaluate_model_filtering(indices_of_accepted_models, 1, NUMBER_OF_BENIGN_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc53510",
   "metadata": {
    "id": "7cc53510"
   },
   "source": [
    "Previously, we only used one adversarial client. For making the models less suspcious, you should now increase the PMR and create more adversarial clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c2a38",
   "metadata": {
    "id": "4e9c2a38"
   },
   "source": [
    "### Increase Poisoned Model Rate (PMR)\n",
    "We train 10 additional poisoned models (on different local datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29413e",
   "metadata": {
    "id": "3e29413e"
   },
   "outputs": [],
   "source": [
    "all_trained_poisoned_models = []\n",
    "all_scaled_malicious_models = []\n",
    "\n",
    "poisoned_datasets = [ColorTriggerBackdoorData(dataset, 0.5,COMPUTATION_DEVICE=COMPUTATION_DEVICE, BACKDOOR_TARGET_CLASS=BACKDOOR_TARGET_CLASS, STD_DEV=STD_DEV, MEAN=MEAN) for dataset in all_training_data[NUMBER_OF_BENIGN_CLIENTS:]]\n",
    "for client_index in range(NUMBER_OF_ADVERSARIES):\n",
    "    print_timed(f'Client {client_index}')\n",
    "    trained_model = train_benign_client(global_model_state_dict, local_model, poisoned_datasets[client_index], printing_prefix='\\t', local_epochs=LOCAL_EPOCHS_FOR_BENIGN_CLIENTS, COMPUTATION_DEVICE=COMPUTATION_DEVICE)\n",
    "    all_trained_poisoned_models.append(trained_model)\n",
    "    scaled_poisoned_model = scale_update(trained_model, (TOTAL_CLIENT_NUMBER / NUMBER_OF_ADVERSARIES) / 2)\n",
    "    all_scaled_malicious_models.append(scaled_poisoned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VQdRbMU1uJEZ",
   "metadata": {
    "id": "VQdRbMU1uJEZ"
   },
   "source": [
    "Evaluate the effectiveness of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65232ec8",
   "metadata": {
    "id": "65232ec8"
   },
   "outputs": [],
   "source": [
    "aggregated_weights = aggregate_models(all_trained_benign_models + all_scaled_malicious_models, global_model_state_dict)\n",
    "aggregated_model.load_state_dict(aggregated_weights)\n",
    "evaluate_model(aggregated_model, test_data=test_data, backdoor_test_data=backdoor_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d642b",
   "metadata": {
    "id": "008d642b"
   },
   "source": [
    "### Check the Defense Again\n",
    "Let's test if the defense still detects the poisoned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1434bc0",
   "metadata": {
    "id": "a1434bc0"
   },
   "outputs": [],
   "source": [
    "indices_of_accepted_models = clustering_defense(all_trained_benign_models + all_scaled_malicious_models)\n",
    "print_timed(f'The filtering accepted to following models: {indices_of_accepted_models}')\n",
    "evaluate_model_filtering(indices_of_accepted_models, len(all_scaled_malicious_models), NUMBER_OF_BENIGN_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda888d5",
   "metadata": {
    "id": "fda888d5"
   },
   "source": [
    "# Problem of KMeans-Based Defenses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca6272",
   "metadata": {
    "id": "03ca6272"
   },
   "source": [
    "So far, the poisoned models are very suspicious and can be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9272af",
   "metadata": {
    "id": "1d9272af"
   },
   "outputs": [],
   "source": [
    "norms_of_benign_updates = [model_dist_norm(benign_model, global_model_state_dict_on_cpu,NAMES_OF_AGGREGATED_PARAMETERS) for benign_model in all_trained_benign_models]\n",
    "norms_of_poisoned_updates = [model_dist_norm(scaled_model, global_model_state_dict_on_cpu,NAMES_OF_AGGREGATED_PARAMETERS) for scaled_model in all_scaled_malicious_models]\n",
    "plt.figure(dpi=300, figsize=(6,1.5))\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS), norms_of_benign_updates, color='#006400', label='Benign $L_2$-Norms')\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS, TOTAL_CLIENT_NUMBER), norms_of_poisoned_updates, color='#B80F0A', label='Scaled $L_2$-Norms')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4fd80",
   "metadata": {
    "id": "e3d4fd80"
   },
   "source": [
    " ### Task 4 : Why KMeans clustering (with predetermined number of clusters) is not effective for the defense?\n",
    "\n",
    " ⏳ **5min**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1f7ed-2e3d-4bb3-b65f-bfa139bac827",
   "metadata": {
    "id": "88d1f7ed-2e3d-4bb3-b65f-bfa139bac827"
   },
   "source": [
    "Double click to edit - write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b3a87-55ad-4f3b-8fe7-0ed6d68d9f83",
   "metadata": {
    "id": "1a4b3a87-55ad-4f3b-8fe7-0ed6d68d9f83"
   },
   "source": [
    "### Solution\n",
    "[Click Here](https://pastebin.com/hNuNMWb8)\n",
    "\n",
    "Password will be given at the timeout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c228f",
   "metadata": {
    "id": "723c228f"
   },
   "source": [
    "### Circumvent K-Means-Based Clustering\n",
    "\n",
    "We craft a mock model, based on the last poisoned model that fools the KMeans-based defense and allows the remaining poisoned models to stay undetected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f7519",
   "metadata": {
    "id": "8a3f7519"
   },
   "outputs": [],
   "source": [
    "mock_model = all_trained_poisoned_models[-1]\n",
    "mock_model = scale_update(mock_model, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fdbb8b",
   "metadata": {
    "id": "a6fdbb8b"
   },
   "outputs": [],
   "source": [
    "# Run the clustering defense\n",
    "indices_of_accepted_models = clustering_defense(all_trained_benign_models + all_scaled_malicious_models[:-1] + [mock_model])\n",
    "print_timed(f'Accepted: {indices_of_accepted_models}')\n",
    "evaluate_model_filtering(indices_of_accepted_models, NUMBER_OF_ADVERSARIES, number_of_benign_clients=NUMBER_OF_BENIGN_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dww_DiMTvFiS",
   "metadata": {
    "id": "dww_DiMTvFiS"
   },
   "source": [
    "As you see every model is now accepted besides the specifically crafted one with index 29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a834ea4",
   "metadata": {
    "id": "1a834ea4"
   },
   "outputs": [],
   "source": [
    "norm_of_mock_update = model_dist_norm(mock_model, global_model_state_dict_on_cpu,NAMES_OF_AGGREGATED_PARAMETERS)\n",
    "plt.figure(dpi=300, figsize=(6,1.5))\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS), norms_of_benign_updates, color='#006400', label='Benign $L_2$-Norms')\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS, TOTAL_CLIENT_NUMBER ), norms_of_poisoned_updates, color='#B80F0A', label='Scaled $L_2$-Norms')\n",
    "plt.bar(TOTAL_CLIENT_NUMBER - 1, norm_of_mock_update, color='r', label='Mock $L_2$-Norm')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecd40f",
   "metadata": {
    "id": "7cecd40f"
   },
   "source": [
    "Note, that you are not able to visualize the benign and scaled models in a linear y-axis scale as the mock model is exaggerately scaled.\n",
    "\n",
    "The figure shows that the scaled models are closer to the benign model than to the mock model, therefore the clustering defense fails here.\n",
    "\n",
    "Note that, an outlier detection based algorithm such as HDBSCAN might fit better here.\n",
    "\n",
    "More info about HDBSCAN:\n",
    "\n",
    "https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058fad0",
   "metadata": {
    "id": "4058fad0"
   },
   "source": [
    "# Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d549124",
   "metadata": {
    "id": "6d549124"
   },
   "source": [
    "Another option to solve this is to restrict the impact of the individual model updates.\n",
    "\n",
    "As for the next task you shall enforce a upper boundary for the $L_2$-norms of the model updates, therefore, restrict the Euclidean distances between the global model and the respective local model.\n",
    "\n",
    "If the $L_2$-norm is higher than the boundary, the model shall be downscaled (clipping).\n",
    "\n",
    "A challenge here is, of course, to determine a suitable clipping boundary: if it is fixed and too low, the aggregated model will be very similar to the global model. Otherwise, if too high, this does not prevent scaling-based attacks.\n",
    "\n",
    "Since the $L_2$ norm values of the models change during the training process, and become smaller when the model converges, we need to carefully select the boundary in a dynamic way.\n",
    "\n",
    "### Task 5: Restrict Euclidean Distance to Global Model\n",
    "\n",
    "⏳ **20min**\n",
    "\n",
    "TASK_TO_DO: Implement the function clip_update_from_model that downscales the given model (weights_of_client) if the Euclidean distance to the global model (the $L_2$-norm of the update) is higher than the given clipping boundary (target_norm). Then automatically determine a suitable clipping boundary and clip all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea51c0",
   "metadata": {
    "id": "16ea51c0"
   },
   "outputs": [],
   "source": [
    "def clip_update_from_model(weights_of_client, target_norm):\n",
    "    \"\"\"\n",
    "    :param weights_of_client: state dict of the model where the L2-norm of the update shall \n",
    "    be restricted\n",
    "    :param target_norm: the maximum allowed norm value\n",
    "    :return state dict of the model with clipped update or the given model if the L2-norm was\n",
    "    already lower than the target norm\n",
    "    \"\"\"\n",
    "    norm = model_dist_norm(weights_of_client, global_model_state_dict_on_cpu,NAMES_OF_AGGREGATED_PARAMETERS)\n",
    "\n",
    "    clipping_factor = min(1, target_norm / norm)\n",
    "    if clipping_factor < 1:\n",
    "        return scale_update(weights_of_client, clipping_factor)\n",
    "    return weights_of_client\n",
    "\n",
    "### IMPLEMENTATION START ###\n",
    "update_norms = [] \n",
    "\n",
    "# HINT\n",
    "# for all all_trained_benign_models + all_scaled_malicious_models we want to compute the distance (norm) with respect to the \n",
    "# global_model_state_dict_on_cpu. Then, compute the median.\n",
    "# For doing so, reuse the function defined above for computing the distance of models.\n",
    "\n",
    "# ...\n",
    "\n",
    "clipped_models = [] \n",
    "\n",
    "# HINT\n",
    "# Now we want to apply the clipping, using the boundary computed above (clipping_boundary).\n",
    "\n",
    "# ...\n",
    "\n",
    "### IMPLEMENTATION END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42647b",
   "metadata": {
    "id": "0d42647b"
   },
   "source": [
    "### Solution\n",
    "[Click Here](https://pastebin.com/8GdsVQ1h)\n",
    "\n",
    "Password will be given at the timeout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb711158",
   "metadata": {
    "id": "bb711158"
   },
   "outputs": [],
   "source": [
    "clipped_update_norms = [model_dist_norm(model, global_model_state_dict_on_cpu,NAMES_OF_AGGREGATED_PARAMETERS) for model in clipped_models]\n",
    "plt.figure(dpi=300, figsize=(6,1.5))\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS), clipped_update_norms[:NUMBER_OF_BENIGN_CLIENTS], color='#006400', label='Clipped Benign $L_2$-Norms')\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS, TOTAL_CLIENT_NUMBER), clipped_update_norms[NUMBER_OF_BENIGN_CLIENTS:], color='#B80F0A', label='Clipped Poisoned $L_2$-Norms')\n",
    "plt.legend(loc='lower center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae0e23",
   "metadata": {
    "id": "ddae0e23"
   },
   "outputs": [],
   "source": [
    "aggregated_weights = aggregate_models(clipped_models, global_model_state_dict)\n",
    "aggregated_model.load_state_dict(aggregated_weights)\n",
    "evaluate_model(aggregated_model, test_data=test_data, backdoor_test_data=backdoor_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947eb869",
   "metadata": {
    "id": "947eb869"
   },
   "source": [
    "An additional advantage of the clipping approach is its resilience against the mock model attack strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf72b2",
   "metadata": {
    "id": "fecf72b2"
   },
   "outputs": [],
   "source": [
    "print_timed(f'Avg L2 Norm of scaled poisoned updates: {np.mean(norms_of_poisoned_updates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3e1b0",
   "metadata": {
    "id": "0dc3e1b0"
   },
   "source": [
    "# Can we do a more efficient attack?\n",
    "### Tune Attack Parameters\n",
    "One strategy to circumvent techniques like clipping is to reduce the $L_2$-norm before scaling the model. An option here is, e.g., to reduce the initial Poisoned Data Rate (PDR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b609a",
   "metadata": {
    "id": "f41b609a"
   },
   "outputs": [],
   "source": [
    "all_scaled_malicious_models_reduced_pdr = []\n",
    "poisoned_datasets_reduced_pdr = [ColorTriggerBackdoorData(dataset, 0.25, COMPUTATION_DEVICE=COMPUTATION_DEVICE, BACKDOOR_TARGET_CLASS=BACKDOOR_TARGET_CLASS, STD_DEV=STD_DEV, MEAN=MEAN) for dataset in all_training_data[NUMBER_OF_BENIGN_CLIENTS:]]\n",
    "for client_index in range(NUMBER_OF_ADVERSARIES):\n",
    "    print_timed(f'Client {client_index}')\n",
    "    trained_model = train_benign_client(global_model_state_dict, local_model, poisoned_datasets_reduced_pdr[client_index], printing_prefix='\\t', COMPUTATION_DEVICE=COMPUTATION_DEVICE, local_epochs=LOCAL_EPOCHS_FOR_BENIGN_CLIENTS)\n",
    "    scaled_poisoned_model = scale_update(trained_model, (TOTAL_CLIENT_NUMBER / NUMBER_OF_ADVERSARIES) / 2)\n",
    "    all_scaled_malicious_models_reduced_pdr.append(scaled_poisoned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4adda",
   "metadata": {
    "id": "1db4adda"
   },
   "outputs": [],
   "source": [
    "norms_of_scaled_model_with_reduced_pdr = [model_dist_norm(global_model_state_dict_on_cpu, m,NAMES_OF_AGGREGATED_PARAMETERS) for m in all_scaled_malicious_models_reduced_pdr]\n",
    "print_timed(f'Avg L2 Norm of scaled poisoned updates: {np.mean(norms_of_scaled_model_with_reduced_pdr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23601499",
   "metadata": {
    "id": "23601499"
   },
   "source": [
    "### Tune Attack Parameters (PDR & LR)\n",
    "Another strategy to reduce the $L_2$-norms of the updates before scaling them is to adapt the learning rate for the training function.\n",
    "\n",
    "We repeat the training with a learning rate of 0.1 and PDR of 25%. Then, we store the trained and scaled models in all_scaled_malicious_models_reduced_pdr_and_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713538f9",
   "metadata": {
    "id": "713538f9"
   },
   "outputs": [],
   "source": [
    "all_scaled_malicious_models_reduced_pdr_and_lr = []\n",
    "\n",
    "poisoned_datasets_reduced_pdr_lr = [ColorTriggerBackdoorData(dataset, 0.25, COMPUTATION_DEVICE=COMPUTATION_DEVICE, BACKDOOR_TARGET_CLASS=BACKDOOR_TARGET_CLASS, STD_DEV=STD_DEV, MEAN=MEAN) for dataset in all_training_data[NUMBER_OF_BENIGN_CLIENTS:]]\n",
    "for client_index in range(NUMBER_OF_ADVERSARIES):\n",
    "    print_timed(f'Client {client_index}')\n",
    "    trained_model = train_benign_client(global_model_state_dict, local_model, poisoned_datasets_reduced_pdr_lr[client_index], lr=0.1, printing_prefix='\\t', local_epochs=LOCAL_EPOCHS_FOR_BENIGN_CLIENTS, COMPUTATION_DEVICE=COMPUTATION_DEVICE)\n",
    "    scaled_poisoned_model = scale_update(trained_model, (TOTAL_CLIENT_NUMBER / NUMBER_OF_ADVERSARIES) / 2)\n",
    "    all_scaled_malicious_models_reduced_pdr_and_lr.append(scaled_poisoned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242bb38",
   "metadata": {
    "id": "1242bb38"
   },
   "outputs": [],
   "source": [
    "norms_of_scaled_model_with_reduced_pdr_and_lr = [model_dist_norm(global_model_state_dict_on_cpu, m,NAMES_OF_AGGREGATED_PARAMETERS) for m in all_scaled_malicious_models_reduced_pdr_and_lr]\n",
    "print_timed(f'Avg L2 Norm of scaled poisoned updates: {np.mean(norms_of_scaled_model_with_reduced_pdr_and_lr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1499ad1",
   "metadata": {
    "id": "a1499ad1"
   },
   "source": [
    "### Evaluate Effectiveness of Tuned Parameters\n",
    "Lets now see, how effective these poisoned models are for injecting the backdoor into the global model.\n",
    "\n",
    "We aggregate the latest poisoned models and the benign models together. We store the aggregated weights in a variable called aggregated_weights_reduced_lr_pdr and load these weights into a model (aggregated model) and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83228225",
   "metadata": {
    "id": "83228225"
   },
   "outputs": [],
   "source": [
    "aggregated_weights_reduced_lr_pdr = aggregate_models(all_trained_benign_models + all_scaled_malicious_models_reduced_pdr_and_lr, global_model_state_dict)\n",
    "aggregated_model.load_state_dict(aggregated_weights_reduced_lr_pdr)\n",
    "evaluate_model(aggregated_model, test_data=test_data, backdoor_test_data=backdoor_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c6eab",
   "metadata": {
    "id": "5b1c6eab"
   },
   "source": [
    "## Constrain-and-Scale attack\n",
    "In the previous parts of this tutorial, we observed that there is a trade-off between attack efficiency and stealthiness. However, it is not convinient to try many different parameters for finding the best trade-off. It is more convinient to let the optimizer used during the training, do this tradeoff. For this, we need to manipulate the loss function. Originally, the loss function just optimizes the performance of the model on the training data. However, now we manipulate it to also keep the malicious model updates inconspicious.\n",
    "\n",
    "This always depends on the used metric. Here, we use the $L_2$-norm between global model and local model for the defense. Therefore, we add a (regularization) term to the loss function, that shall keep this distance small. The new term is called anomaly-evasion loss, while the old loss function is called now class loss. Further, we introduce a parameter $\\alpha$ to weight both terms for combining them.\n",
    "\n",
    "\n",
    "\n",
    "### Implement Defense Evasion\n",
    "We use the function `train_malicious_client`, defined as:\n",
    "```python\n",
    "def train_malicious_client(global_model_state_dict, local_model, local_training_data, COMPUTATION_DEVICE, local_epochs, NAMES_OF_AGGREGATED_PARAMETERS,\n",
    "      lr=0.2, printing_prefix='', alpha=0.6,)\n",
    "```\n",
    "We train the malicious clients (`NUMBER_OF_ADVERSARIES`) using the Adversarial Loss, implemented [here](https://github.com/MarcoChilese/TUDASummerSchool22Code/blob/0f96a1aaf6311d482b206f02440cef97381953a4/TUDASummerSchool22/TrainingUtils.py#L65).\n",
    "\n",
    "\n",
    "Once the models are trained we scale them according to a factor determined as $\\frac{\\text{TOTAL_CLIENT_NUMBER}}{\\text{NUMBER_OF_ADVERSARIES}}\\frac12$.\n",
    "\n",
    "[5] http://proceedings.mlr.press/v108/bagdasaryan20a/bagdasaryan20a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df65df",
   "metadata": {
    "id": "f7df65df"
   },
   "outputs": [],
   "source": [
    "# Anomaly Evasion (2)\n",
    "all_trained_poisoned_models_ae = [] \n",
    "all_scaled_poisoned_models_ae = [] \n",
    "\n",
    "poisoned_datasets = [ColorTriggerBackdoorData(dataset, 0.5, COMPUTATION_DEVICE=COMPUTATION_DEVICE, BACKDOOR_TARGET_CLASS=BACKDOOR_TARGET_CLASS, STD_DEV=STD_DEV, MEAN=MEAN) for dataset in all_training_data[NUMBER_OF_BENIGN_CLIENTS:]] \n",
    "\n",
    "for client_index in range(NUMBER_OF_ADVERSARIES):\n",
    "    print_timed(f'Client {client_index}')\n",
    "    trained_model = train_malicious_client(global_model_state_dict, local_model, poisoned_datasets[client_index], lr=.1,\n",
    "                                           printing_prefix='\\t', COMPUTATION_DEVICE=COMPUTATION_DEVICE, local_epochs=2, NAMES_OF_AGGREGATED_PARAMETERS=NAMES_OF_AGGREGATED_PARAMETERS) \n",
    "    all_trained_poisoned_models_ae.append(trained_model)\n",
    "    scaled_poisoned_model = scale_update(trained_model, (TOTAL_CLIENT_NUMBER / NUMBER_OF_ADVERSARIES) / 2)\n",
    "    all_scaled_poisoned_models_ae.append(scaled_poisoned_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb4135",
   "metadata": {
    "id": "5aeb4135",
    "tags": []
   },
   "source": [
    "### Taks 6: Implement Dynamic Scaling\n",
    "\n",
    "⏳ **25min**\n",
    "\n",
    "\n",
    "TASK_TO_DO: You need to repeat the training and during the training, you should consider the anomaly evasion loss. You then choose the scaling factor $\\gamma$ such that the models have an inconspicious $L_2$-norm. For simplicity, you should use the actual $L_2$-norms of the benign updates (denoted as $S$ in the following equation):\n",
    "\n",
    "$$\\gamma=\\frac{S}{||W-G_{t-1}||_2}$$\n",
    "\n",
    "For training a malicious client you should use the following function:\n",
    "\n",
    "```python\n",
    "def train_malicious_client(global_model_state_dict, local_model, dataset,\n",
    "                         lr, local_epochs, alpha,\n",
    "                       COMPUTATION_DEVICE,NAMES_OF_AGGREGATED_PARAMETERS)\n",
    "```\n",
    "Check the full implementation [here](https://github.com/MarcoChilese/TUDASummerSchool22Code/blob/0f96a1aaf6311d482b206f02440cef97381953a4/TUDASummerSchool22/TrainingUtils.py#L42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e79b6d",
   "metadata": {
    "id": "28e79b6d"
   },
   "outputs": [],
   "source": [
    "all_scaled_malicious_models_anomaly_evasion = [] \n",
    "\n",
    "target_norm = np.mean(norms_of_benign_updates) \n",
    "poisoned_datasets_reduced_pdr_ = [ColorTriggerBackdoorData(dataset, 0.90, COMPUTATION_DEVICE=COMPUTATION_DEVICE, BACKDOOR_TARGET_CLASS=BACKDOOR_TARGET_CLASS, STD_DEV=STD_DEV,MEAN=MEAN) for dataset in all_training_data[NUMBER_OF_BENIGN_CLIENTS:]] #given\n",
    "\n",
    "# HINT\n",
    "# for each adversarial client train a poisoned model using train_malicious_client function\n",
    "# for Training use: poisoned_datasets_reduced_pdr_, lr=0.5, local_epochs=1, alpha=0.55\n",
    "# calculate the scaling factor and scale up the models accordingly\n",
    "\n",
    "### IMPLEMENTATION START ###\n",
    "# for <complete>\n",
    "    print_timed(f'Client {client_index}')\n",
    "    \n",
    "    trained_model = <complete>\n",
    "    \n",
    "    unscaled_norm = <complete>\n",
    "    \n",
    "    scaling_factor = <complete>\n",
    "    \n",
    "    print_timed(f'\\tScaling Factor: {scaling_factor}') # given \n",
    "    \n",
    "    scaled_poisoned_model = <complete>\n",
    "    print_timed(f'\\tL2 Norm: {model_dist_norm(global_model_state_dict_on_cpu, scaled_poisoned_model, NAMES_OF_AGGREGATED_PARAMETERS)}') # given\n",
    "    \n",
    "### IMPLEMENTATION END ###\n",
    "\n",
    "aggregated_weights = aggregate_models(all_trained_benign_models + all_scaled_malicious_models_anomaly_evasion, global_model_state_dict)\n",
    "aggregated_model.load_state_dict(aggregated_weights)\n",
    "evaluate_model(aggregated_model, test_data=test_data, backdoor_test_data=backdoor_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460a547",
   "metadata": {
    "id": "5460a547"
   },
   "source": [
    "### Solution\n",
    "[Click Here](https://pastebin.com/6kELecJE)\n",
    "\n",
    "Password will be given at the timeout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1afd8a",
   "metadata": {
    "id": "dd1afd8a"
   },
   "outputs": [],
   "source": [
    "norms_of_evaision_poisoned_updates = [model_dist_norm(scaled_model, global_model_state_dict_on_cpu, NAMES_OF_AGGREGATED_PARAMETERS) for scaled_model in all_scaled_malicious_models_anomaly_evasion]\n",
    "plt.figure(dpi=300, figsize=(6,1.5))\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS), norms_of_benign_updates, color='#006400', label='Benign $L_2$-Norms')\n",
    "plt.bar(np.arange(NUMBER_OF_BENIGN_CLIENTS, TOTAL_CLIENT_NUMBER), norms_of_evaision_poisoned_updates, color='#B80F0A', label='Scaled $L_2$-Norms')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c4c51",
   "metadata": {
    "id": "026c4c51"
   },
   "source": [
    "Now, the $L_2$-norms of the model updates are inconspicious!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3888ede",
   "metadata": {
    "id": "d3888ede"
   },
   "outputs": [],
   "source": [
    "indices_of_accepted_models_ae2 = clustering_defense(all_trained_benign_models + all_scaled_malicious_models_anomaly_evasion)\n",
    "print_timed(f'The filtering accepted to following models: {indices_of_accepted_models_ae2}')\n",
    "evaluate_model_filtering(indices_of_accepted_models_ae2, number_of_benign_clients=NUMBER_OF_BENIGN_CLIENTS,number_of_adversaries=NUMBER_OF_ADVERSARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc9a0e",
   "metadata": {
    "id": "07fc9a0e"
   },
   "source": [
    "Clustering is still not able to detect these updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864b539",
   "metadata": {
    "id": "1864b539"
   },
   "source": [
    "### Any Effective defense? \n",
    "\n",
    "We use the above code for clipping all updates (after dynamically caclulating the clipping boundary). For clipping and aggregation, we use only the models that were accepted by the clustering but calculate the clipping boundary based on all model updates. Then, we aggregate the models and play around with the attack parameters and try to maximize the BA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7877961",
   "metadata": {
    "id": "a7877961"
   },
   "outputs": [],
   "source": [
    "update_norms = [model_dist_norm(m, global_model_state_dict_on_cpu, NAMES_OF_AGGREGATED_PARAMETERS) for m in tqdm(all_trained_benign_models + all_scaled_malicious_models_anomaly_evasion)]\n",
    "clipping_boundary = np.median(update_norms)\n",
    "all_models_ae2 = all_trained_benign_models + all_scaled_malicious_models_anomaly_evasion\n",
    "all_accepted_models = [all_models_ae2[i] for i in indices_of_accepted_models_ae2]\n",
    "clipped_models = [clip_update_from_model(m, clipping_boundary) for m in all_accepted_models]\n",
    "aggregated_weights_clipped = aggregate_models(clipped_models, global_model_state_dict)\n",
    "aggregated_model.load_state_dict(aggregated_weights_clipped)\n",
    "evaluate_model(aggregated_model, test_data=test_data, backdoor_test_data=backdoor_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d6ba2",
   "metadata": {
    "id": "5e6d6ba2"
   },
   "source": [
    "If an effective defense mechanism is not deployed, it might happen that the defense wrongly excludes more benign models than poisoned ones. For example, a bad defense might reject many benign models but accepts most of the poisoned models. Therefore, an ineffective defense can actually increase the PMR meaning that the BA increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756aaff",
   "metadata": {
    "id": "6756aaff"
   },
   "source": [
    "The (naive) combination of above techniques is not effective against advanced backdoor hiding techniques. If you are interested in this topic, you can try to find a more effective combination of different techniques.\n",
    "\n",
    "You should also checkout some recent papers on poisoning attacks in FL:\n",
    "* Nguyen, Thien Duc, et al. \"FLAME: Taming Backdoors in Federated Learning.\" To appear at USENIX Security 2022.\n",
    "    * https://www.usenix.org/system/files/sec22fall_nguyen.pdf\n",
    "* Rieger, Phillip, et al. \"DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection.\" NDSS 2022.\n",
    "    * https://www.ndss-symposium.org/wp-content/uploads/2022-156-paper.pdf\n",
    "* Naseri, Mohammad, Jamie Hayes, and Emiliano De Cristofaro. \"Local and central differential privacy for robustness and privacy in federated learning.\" NDSS 2022.\n",
    "    * https://www.ndss-symposium.org/wp-content/uploads/2022-54-paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d3cab",
   "metadata": {
    "id": "2b6d3cab"
   },
   "source": [
    "If you are interested in this topic and want to discuss it further, please contact us:\n",
    "* hossein.fereidooni@trust.tu-darmstadt.de\n",
    "* phillip.rieger@trust.tu-darmstadt.de\n",
    "* marco.chilese@trust.tu-darmstadt.de\n",
    "* ahmad.sadeghi@trust.tu-darmstadt.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96115a2e",
   "metadata": {
    "id": "96115a2e"
   },
   "source": [
    "## And do not forget:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8b235",
   "metadata": {
    "id": "6eb8b235"
   },
   "source": [
    "![](https://media-exp1.licdn.com/dms/image/C4E22AQH6zr3T0UW0bQ/feedshare-shrink_800/0/1592720369091?e=1657152000&v=beta&t=sWwkNyhr-Z5pAQ4omzl33jKknqVPn8M4Sz0VTtiemTk \"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TUDA_FL_Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
